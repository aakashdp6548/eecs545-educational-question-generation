# Answer-Agnostic Question Generation for Educational Questions

This repository contains the code for the EECS 545 W23 project "Answer-Agnostic Question Generation for Educational Questions". In this project, we aim to investigate question generation models that do not require an answer at train time, hence "answer-agnostic".

We experiment with three different models:
1. T5, a transformer-based model
2. Graph2Seq, a graph neural network
3. ChatGPT

## Data
We use the LearningQ dataset, which consists of educational questions from Khan Academy and TedEd. The raw data can be found on the LearningQ github repo. We include some preprocessing data in ``data``, and functions used to preprocess the raw data in ``preprocessing/utils.py``.

## T5
The code for the T5 models can be found under the subfolders ``custom_ft_t5_qg``, which contains the multi-task and end-to-end inference models, and ``finetuned_t5_qg``, which contains the finetuning code.

## Graph2Seq
The code for the Graph2Seq model can be found under the ``g2s_question_generation`` subfolder. The code requires CoreNLP to be installed and running. ``main.py`` contains the main training code, and ``inference_advance.py`` contains the code for evaluation and inference.

## ChatGPT
The code for the summarizer + ChatGPT is under ``summarizer_chatgpt``. 

## Evaluation
The code for calculating the BLEU, ROUGE, and METEOR metrics is in ``evaluate.py``.

### Evaluation Results
| Model | BLEU-1 | BLEU-4 | ROUGE-L | METEOR | C | R | OE |
|-------|--------|--------|---------|--------|---|---|----|
|T5 End-To-End | **0.4885** | **0.1053** | **0.2624** | 0.2358 | 4.37 | 3.25 | 0.18 |
|T5 Multitask | 0.4740 | 0.0659 | 0.2476 | 0.2171 | 4.0 | 3.63 | 0.02 |
|T5 Finetuning | 0.4702 | 0.0824 | 0.2581 | **0.2699** | 4.4 | 3.5 | 0.14 |
|G2S (no answer) | 0.1593 | 0.0270 | 0.2278 | 0.0723 | 2.2 | 1.8 | 0.10 |
|G2S (generated) | 0.1819 | 0.0230 | 0.2377 | 0.0762 | 2.67 | 2.6 | 0.12 |
|G2S (with SQuAD)| 0.3303 | 0.0758 | 0.2320 | 0.0725 | 2.8 | 3.0 | 0.16 |
|0-shot ChatGPT | 0.4585 | 0.0 | 0.2287 | 0.2641 | **5** | **5** | **1** |


### Case Study
Sample questions generated by each model on a context paragraph taken from a class lecture transcript about RNNs and attention. The questions are arranged in descending order of human evaluation score.

**Context passage:** So generally speaking, RNN encoder decoder architecture is not very efficient for handling long sequence because it’s hard to remember every detail that’s in the input. So much better mechanism for handling this variable length is to use the notion of attention mechanism.
| Model | Generated Question |
|----|------|
|**ChatGPT:** | What is the advantage of using the attention mechanism over the RNN encoder-decoder architecture for handling long sequences? |
|**T5 MultiTask:** | Why is RNN encoder not very efficient for handling long sequences? |
|**T5 E2E:** | Why is RNN encoder decoder architecture not very efficient for handling long sequence? |
|**T5 Finetuned:** | Why is it so hard to remember every detail that’s in the input? |
**G2S w/SQuAD:** | What is the notion of the encoder? |
**G2S (only TedEd):** | What is the sequence of the sequence of the input. architecture? |